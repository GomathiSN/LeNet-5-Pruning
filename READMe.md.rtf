{\rtf1\ansi\ansicpg1252\cocoartf2761
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 LeNet-5 Pruning on MNIST Dataset\
\
This project explores deep neural network compression through pruning. The LeNet-5 model was pruned to reduce computational complexity while maintaining high accuracy on the MNIST dataset.\
\
Abstract\
\
Deep neural network compression is crucial for deploying models on embedded systems with limited hardware resources. This project focuses on pruning a LeNet-5 network on the MNIST dataset to achieve a high compression ratio with minimal performance loss. The approach utilized includes pruning, retraining, and fine-tuning the network to ensure optimal performance.\
\
Project Structure\
\
LeNet-5-Pruning/\
\uc0\u9474 \
\uc0\u9500 \u9472 \u9472  DL_Final_Report.pdf     # Detailed report on the project\
\uc0\u9500 \u9472 \u9472  final_code/             # Directory containing the project code\
\uc0\u9500 \u9472 \u9472  result.json             # JSON file containing the initial training results\
\uc0\u9500 \u9472 \u9472  results.json            # JSON file containing the results after pruning\
\uc0\u9500 \u9472 \u9472  README.md               # Project documentation\
\
Installation\
\
To run this project, you need to have Python and the necessary libraries installed. \
\
Data Description\
\
The project uses the MNIST dataset, a widely recognized collection of 70,000 grayscale images of handwritten digits (28x28 pixels each). The dataset is split into a training set of 60,000 samples and a test set of 10,000 samples.\
\
Methodology\
\
The pruning process involves the following steps:\
1. Threshold Identification: Determine the sparsity ratio, i.e., the percentage of weights to prune.\
2. Mask Generation: Create a mask tensor to retain the essential weights.\
3. Model Retraining: Retrain the pruned model to fine-tune the remaining connections.\
\
The model was trained with a batch size of 128 over 10 epochs, followed by 5 epochs of fine-tuning. The pruning targeted a sparsity ratio of 0.5, meaning 50% of the model's weights were pruned.\
\
Results\
\
Pre-Pruning Results:\
- Initial Loss: 0.1513\
- Initial Accuracy: 95%\
\
 Post-Pruning and Fine-Tuning:\
- Final Loss: 0.0367\
- Final Accuracy: 98.87%\
\
The final sparsity of the model is 0.5, indicating that 50% of the weights were removed, yet the model maintained a high accuracy, demonstrating the effectiveness of pruning in reducing computational complexity without sacrificing performance.\
\
## Usage\
\
1. Training the Model: Use the provided scripts to train the LeNet-5 model on the MNIST dataset.\
2. Pruning the Model: Apply the pruning methods to compress the model while retaining accuracy.\
3. Evaluation: Evaluate the pruned model's performance and compare it to the original model.\
\
References\
\
- Han, S., Pool, J., Tran, J., & Dally, W. J. (2015). Learning both weights and connections for efficient neural networks. *arXiv preprint arXiv:1506.02626*.\
- LeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. *Proceedings of the IEEE*.\
\
 Authors\
\
Gomathi Shobha Nagaraja\
}